\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{longtable} % For the large table
\usepackage{array}

\geometry{a4paper, margin=1in}

\title{Integral Solution Analysis 2}
\author{}
\date{}

\begin{document}

\maketitle

\section*{Comprehensive Integral Solution Analysis: \(\int \frac{\tan^3(\ln x)}{x} dx\)}

This document presents a technical evaluation of 23 Large Language Model (LLM) versions on their ability to solve the indefinite integral:
\[
\int \frac{\tan^3(\ln x)}{x} \, dx
\]

\textbf{Correct Solution:}
\[
\frac{1}{2}\tan^2(\ln x) + \ln|\cos(\ln x)| + C
\]
\textit{(Equivalent forms using \(\sec^2(\ln x)\) or \(-\ln|\sec(\ln x)|\) are also accepted).}

\section{Master Results Table}

\begin{longtable}{@{}p{3.5cm}llp{1.5cm}p{5.5cm}@{}}
\toprule
\textbf{Model} & \textbf{Size} & \textbf{Architecture} & \textbf{Status} & \textbf{Key Failure/Success Factor} \\ \midrule
\endhead

\textbf{Gemma 3 1B} & 1B & Dense (MatFormer) & \textcolor{red}{\textbf{Fail}} & Hallucinated algebraic identities due to compression. \\
\textbf{Gemma 3 4B} & 4B & Dense (MatFormer) & \textcolor{orange}{\textbf{Partial}} & Correct math, but context window failure (forgot \(u\) substitution). \\
\textbf{Gemma 3 12B} & 12B & Dense (MatFormer) & \textcolor{green}{\textbf{Pass}} & The stability threshold for the Gemma 3 family. \\
\textbf{Gemma 3 27B} & 27B & Dense (MatFormer) & \textcolor{green}{\textbf{Pass}} & Robust performance; effectively used \(\sec^2\) identity. \\
\textbf{Mistral 7B} & 7B & Dense & \textcolor{green}{\textbf{Pass}} & Outperformed Llama 70B via strict instruction following. \\
\textbf{Nemotron Nano 9B v2} & 9B & Hybrid (Mamba-2) & \textcolor{green}{\textbf{Pass}} & \textbf{Mamba Architecture} maintained state perfectly. \\
\textbf{Nemotron Nano 12B VL} & 12.6B & Hybrid (Mamba-2) & \textcolor{green}{\textbf{Pass}} & \textbf{Best Verification:} Rejected hallucinated calculator data. \\
\textbf{Trinity Mini} & 26B & Sparse MoE (\textbf{3B Active}) & \textcolor{red}{\textbf{Fail}} & \textbf{Routing Failure:} 3B active parameters dropped the sign context. \\
\textbf{Tongyi DeepResearch} & 30.5B & Sparse MoE (\textbf{3.3B Active}) & \textcolor{green}{\textbf{Pass}} & Agentic training compensated for low active parameter count. \\
\textbf{Olmo 3 32B Think} & 32B & Dense (RLVR) & \textcolor{green}{\textbf{Pass}} & \textbf{Metacognition:} Explicitly doubted and verified itself. \\
\textbf{Llama 3.3 70B} & 70B & Dense & \textcolor{red}{\textbf{Fail}} & \textbf{Rote Error:} Failed power rule integration (\(\int x \neq x\)). \\
\textbf{Cognito Llama 70B} & 70B & Llama Fine-tune & \textcolor{red}{\textbf{Fail}} & ``Reasoning'' fine-tune caused logical over-complication. \\
\textbf{Cognito Llama 405B} & 405B & Llama Fine-tune & \textcolor{green}{\textbf{Pass}} & Scale corrected the logic errors seen in the 70B version. \\
\textbf{GLM 4.5 Air} & 106B & MoE (\textbf{12B Active}) & \textcolor{green}{\textbf{Pass}} & Sufficient active memory for symbolic logic. \\
\textbf{Devstral 2 2512} & 123B & Dense (Coding) & \textcolor{green}{\textbf{Pass}} & \textbf{Coding Specialist:} Treated math like syntax parsing. \\
\textbf{Nova 2 Lite} & <20B & Distilled & \textcolor{green}{\textbf{Pass}} & Efficient knowledge distillation from larger teacher. \\
\textbf{KAT-Coder-Pro V1} & \(\sim\)40B & Agentic Coding & \textcolor{green}{\textbf{Pass}} & Structured the solution as a ``Plan of Action''. \\
\textbf{LongCat Flash} & 560B & MoE (\textbf{27B Active}) & \textcolor{green}{\textbf{Pass}} & ``Flash-Thinking'' dynamic compute ensured accuracy. \\
\textbf{DeepSeek V3.1 Nex N1} & 671B & MoE (\textbf{37B Active}) & \textcolor{green}{\textbf{Pass}} & Agentic post-training verified preconditions (\(dx/x\)). \\
\textbf{DeepSeek R1T Chimera} & 671B & MoE (Reasoning) & \textcolor{green}{\textbf{Pass}} & Verified result via differentiation. \\
\textbf{DeepSeek R1T2 Chimera} & 671B & MoE (Reasoning) & \textcolor{green}{\textbf{Pass}} & \textbf{Tri-Mind Arch:} Fixed consistency issues of V1. \\
\textbf{Kimi K2} & 1T & MoE (\textbf{32B Active}) & \textcolor{green}{\textbf{Pass}} & \textbf{Verified:} Differentiated result to check correctness. \\
\textbf{Grok 4.1 Fast} & Unk & Agentic & \textcolor{green}{\textbf{Pass}} & Fast, accurate derivation. \\ \bottomrule
\end{longtable}

\section{Detailed Model Analysis}

\subsection{The Gemma 3 Family (1B - 27B)}
\begin{itemize}
    \item \textbf{Gemma 3 1B:} \textbf{Failed.} The 1B model, optimized for mobile deployment, displayed ``hallucination under load.'' It correctly identified the \(u\)-substitution but invented algebraic identities to force a simplification that didn't exist.
    \item \textbf{Gemma 3 4B:} \textbf{Partial.} This model solved the calculus correctly but failed the ``Instruction Constraint'' (Substitute back to \(x\)). This is a classic ``Context Window Failure'' common in \(<7\)B models, where the initial constraint is overwritten by the cognitive load of the math steps.
    \item \textbf{Gemma 3 12B \& 27B:} \textbf{Success.} The 12B model appears to be the \textbf{Minimum Viable Size} for reliable symbolic calculus in the Gemma architecture, showing none of the drift present in the smaller variants.
\end{itemize}

\subsection{Hybrid Mamba vs. Sparse MoE (The ``Small'' Model War)}
\begin{itemize}
    \item \textbf{Nemotron Nano 12B 2 VL (Hybrid Mamba):} \textbf{Star Performer.} This model uses a \textbf{Hybrid Transformer-Mamba} architecture. The Mamba (State Space Model) component acts as a recurrent memory, allowing it to ``hold'' the variable definitions (\(u=\ln x\)) indefinitely without the compute cost of attention. It uniquely \textbf{rejected a hallucinated calculator result} during its reasoning trace, trusting its own derivation instead.
    \item \textbf{Trinity Mini (Sparse MoE):} \textbf{Failed.} While having 26B total parameters, it only uses \textbf{3B Active Parameters} per token. This proved insufficient. The ``Math Expert'' layer likely lost the negative sign context during routing, a known weakness of Sparse MoEs when the active parameter count drops below \(\sim\)7B.
    \item \textbf{Tongyi DeepResearch (Sparse MoE):} \textbf{Success.} Despite also having only \textbf{3.3B Active Parameters}, it succeeded. Its training on \textbf{Long-Horizon Agentic Research} enforces a ``step-by-step verification'' protocol that Trinity lacks, effectively brute-forcing accuracy through rigid logical structure.
\end{itemize}

\subsection{The ``Thinking'' Models (RLVR \& Reasoning)}
\begin{itemize}
    \item \textbf{Olmo 3 32B Think:} \textbf{Success.} This model is trained with \textbf{Reinforcement Learning with Verifiable Rewards (RLVR)}. Its transcript was unique: it explicitly ``doubted'' its own sign convention for \(\int \tan u\), paused, and chose to differentiate the result to be sure. It proves that a 32B ``Thinker'' can outperform a 70B ``Predictor.''
    \item \textbf{DeepSeek R1T2 Chimera:} \textbf{Success.} An ``Assembly of Experts'' merging the reasoning of R1 with the chat fluency of V3. It fixed the ``over-thinking'' loops sometimes seen in R1T, arriving at the solution efficiently while still verifying it via differentiation.
\end{itemize}

\subsection{Large Dense Models (70B+)}
\begin{itemize}
    \item \textbf{Llama 3.3 70B:} \textbf{Failed.} A significant failure for a 70B model. It attempted to integrate \(\int x \, dx\) but failed the power rule, a sign that its training data prioritized ``chat fluency'' over ``mathematical precision''.
    \item \textbf{Devstral 2 2512 (123B):} \textbf{Success.} As a \textbf{Coding-Optimized} model, Devstral treated the integral like a syntax parsing problem. It broke the ``code'' (equation) into functions (substitution, identity), executed them, and returned the output with zero ``fluff''.
\end{itemize}

\subsection{Massive Scale MoEs (>500B)}
\begin{itemize}
    \item \textbf{Kimi K2 (1T Parameters):} \textbf{Success.} With 1 Trillion parameters, Kimi K2 has highly specialized experts. It likely routed this prompt to a specific ``Latex/Calculus'' expert trained on arXiv papers, resulting in the most formally mathematically correct notation of the group.
    \item \textbf{DeepSeek V3.1 Nex N1:} \textbf{Success.} This ``Agentic'' variant of V3 structured its response as a \textbf{Workflow}. It validated the ``Precondition'' (\(dx/x\) exists) before executing the ``Action'' (Substitution), a behavior derived from its autonomous agent post-training.
\end{itemize}

\section{Conclusion}

The analysis of these 23 models highlights three critical trends in 2025 AI architecture:

\begin{enumerate}
    \item \textbf{Hybrid Mamba Wins:} The \textbf{Nemotron 12B} (Hybrid) verified its answer better than the \textbf{Llama 70B} (Dense), proving that State Space Models are superior for logical state retention.
    \item \textbf{Active Parameter Threshold:} For MoEs, \textbf{12B Active Parameters} (GLM 4.5, Kimi) is the safe zone. Dipping to \textbf{3B Active} (Trinity) risks logic errors unless compensated by specific Agentic training (Tongyi).
    \item \textbf{Metacognition:} The strongest models (Olmo Think, DeepSeek R1, Kimi) all shared one trait: \textbf{Self-Verification}. They differentiated their answers to check for errors, a step the older-style models (Llama 3.3) failed to take.
\end{enumerate}

\end{document}
